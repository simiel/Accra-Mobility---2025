{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q1uHOfbuNWDJ"
      },
      "source": [
        "This is a simple starter notebook to help you get started with the hackathon\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YMt3I2pgP76M"
      },
      "outputs": [],
      "source": [
        "# import libraries\n",
        "import os\n",
        "import random\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "from datetime import datetime\n",
        "\n",
        "%matplotlib inline\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Linear models\n",
        "from sklearn.linear_model import (\n",
        "    LinearRegression, Ridge, Lasso, ElasticNet,\n",
        "    BayesianRidge, ARDRegression,\n",
        "    HuberRegressor, RANSACRegressor, TheilSenRegressor,\n",
        "    SGDRegressor,\n",
        "    PoissonRegressor, GammaRegressor, TweedieRegressor\n",
        ")\n",
        "\n",
        "# Ensemble models\n",
        "from sklearn.ensemble import (\n",
        "    RandomForestRegressor, GradientBoostingRegressor,\n",
        "    HistGradientBoostingRegressor, AdaBoostRegressor,\n",
        "    BaggingRegressor, ExtraTreesRegressor,\n",
        "    StackingRegressor, VotingRegressor\n",
        ")\n",
        "\n",
        "# Tree-based model\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "\n",
        "# Support Vector Regression\n",
        "from sklearn.svm import SVR, LinearSVR\n",
        "\n",
        "# Nearest Neighbors\n",
        "from sklearn.neighbors import KNeighborsRegressor, RadiusNeighborsRegressor\n",
        "\n",
        "# Gaussian Process Regression\n",
        "from sklearn.gaussian_process import GaussianProcessRegressor\n",
        "\n",
        "# Neural Networks\n",
        "from sklearn.neural_network import MLPRegressor"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J2DhGqHpQkhk"
      },
      "outputs": [],
      "source": [
        "# Set seed for reproducability\n",
        "SEED = 2025\n",
        "random.seed(SEED)\n",
        "np.random.seed(SEED)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ro-KRpqn5pbB"
      },
      "source": [
        "\n",
        "## Loading and previewing data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EX2hr8mGQruk"
      },
      "outputs": [],
      "source": [
        "DATA_PATH = 'data'\n",
        "SUBMISSION_PATH = 'submissions'\n",
        "# Load files\n",
        "train = pd.read_csv(os.path.join(DATA_PATH, 'Train.csv'))\n",
        "test = pd.read_csv(os.path.join(DATA_PATH, 'Test.csv'))\n",
        "samplesubmission = pd.read_csv(os.path.join(SUBMISSION_PATH, 'SampleSubmission.csv'))\n",
        "weather_df = pd.read_csv(os.path.join(DATA_PATH, 'Accra_weather.csv'), index_col=0)\n",
        "variable_def = pd.read_csv(os.path.join(DATA_PATH, 'VariableDefinitions.csv'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 551
        },
        "id": "1p0IORz3pG1L",
        "outputId": "b24c6fcf-81b5-4f48-9b11-b725d911a041"
      },
      "outputs": [],
      "source": [
        "variable_def"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 226
        },
        "id": "wY-QM-6Jc3gB",
        "outputId": "a9223ca9-e0fd-455b-b2e6-ed3fc332ebca"
      },
      "outputs": [],
      "source": [
        "train.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 226
        },
        "id": "aizfGRd6c5bW",
        "outputId": "23eb9526-fc47-4502-f1d1-b5c3b6ae9c9a"
      },
      "outputs": [],
      "source": [
        "test.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "ditPO4DMc7st",
        "outputId": "80518b1a-3c07-4a45-916c-e60861fadec0"
      },
      "outputs": [],
      "source": [
        "samplesubmission.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 237
        },
        "id": "WsQvXHmdc9u2",
        "outputId": "9fe5aa58-fa60-47bb-9ae3-9c146374b8ec"
      },
      "outputs": [],
      "source": [
        "weather_df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ag0lQcSuTrfZ"
      },
      "outputs": [],
      "source": [
        "def extract_datetime_features(data, cols: list):\n",
        "    df = data.copy()\n",
        "    for col in cols:\n",
        "        df[col] = pd.to_datetime(df[col])\n",
        "        df[f'{col}_hour'] = df[col].dt.hour\n",
        "        df[f'{col}_day'] = df[col].dt.day\n",
        "        df[f'{col}_month'] = df[col].dt.month\n",
        "\n",
        "    return df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 226
        },
        "id": "J_SKQjDCSTwh",
        "outputId": "3d21c9ee-f6b7-4c61-fc87-9300395eae39"
      },
      "outputs": [],
      "source": [
        "# extract datetime features\n",
        "train = extract_datetime_features(train, ['lcl_start_transporting_dttm'])\n",
        "test = extract_datetime_features(test, ['lcl_start_transporting_dttm'])\n",
        "weather_df = extract_datetime_features(weather_df, ['lcl_datetime'])\n",
        "\n",
        "# Preview train dataset\n",
        "train.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 226
        },
        "id": "zOBgdgcPRTje",
        "outputId": "4f18e32a-8a3c-41a3-fa72-ec775c030fa4"
      },
      "outputs": [],
      "source": [
        "# Preview test dataset\n",
        "test.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "h3CsH4JkRQwt",
        "outputId": "c5a14209-a0cd-48d6-901e-c72cdb83bd2c"
      },
      "outputs": [],
      "source": [
        "# Preview sample submission file\n",
        "samplesubmission.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 344
        },
        "id": "uM8nA_t9RVcc",
        "outputId": "e26e3b84-97bd-4268-860e-e9bc7128e939"
      },
      "outputs": [],
      "source": [
        "# Preview graph data\n",
        "weather_df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WRLIKYLiRY1Q",
        "outputId": "b48a2f98-f73c-4fb2-fdb1-a7fb994b9c9d"
      },
      "outputs": [],
      "source": [
        "# Check size and shape of datasets\n",
        "train.shape, test.shape, samplesubmission.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LkRxj0NZRa3K",
        "outputId": "318459dd-55b2-445d-a71c-5c6ab07249d9"
      },
      "outputs": [],
      "source": [
        "# Train to test sets ratio\n",
        "(test.shape[0]) / (train.shape[0] + test.shape[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KNtwMcKjZk9x"
      },
      "source": [
        "\n",
        "## Statistical summaries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 320
        },
        "id": "NNmTMedHRcEa",
        "outputId": "30fbafbb-14cc-407b-dc57-29e61c678123"
      },
      "outputs": [],
      "source": [
        "# Train statistical summary\n",
        "train.describe(include = 'number')  # could also do include = 'all' or 'object'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2kESZ-qvnSEa"
      },
      "source": [
        "Some insights from the above summary:\n",
        " - The train data provided has 57 596 data points\n",
        " - The average trip time is 10.08 mins\n",
        " - The shortest trip took a 1.02 minute while the longest took 585.93 mins (possible outliers)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 689
        },
        "id": "P3AlXynlXziW",
        "outputId": "55046dd3-71a1-4110-d1fa-0fb6cb4020ce"
      },
      "outputs": [],
      "source": [
        "# Target variable distribution\n",
        "sns.set_style('darkgrid')\n",
        "plt.figure(figsize = (13, 7))\n",
        "sns.histplot(train.Target)\n",
        "plt.title('Target variable distribution', y = 1.02, fontsize = 15)\n",
        "display(plt.show(), train.Target.skew())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x4gly1_uSAeY"
      },
      "source": [
        "The target variable appears to be right skewed\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YZ6dymKo_VXp"
      },
      "source": [
        "\n",
        "## Outliers\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 611
        },
        "id": "N8d1_h8_RhCP",
        "outputId": "8fb079f5-a8ca-4af0-a16f-67fddb4b9b25"
      },
      "outputs": [],
      "source": [
        "# Plotting boxplot for travel time\n",
        "sns.set_style('darkgrid')\n",
        "plt.figure(figsize = (13, 7))\n",
        "sns.boxplot(train.Target)\n",
        "plt.title('Boxplot showing travel time outliers', y = 1.02, fontsize = 15)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GvtoMmLdak_z"
      },
      "source": [
        "Outliers are those data points which differ significantly from other observations present in given dataset.\n",
        "\n",
        "Suggestions on how to handle outliers:\n",
        " - Transforming the outliers by scaling - log transformation, box-cox transformation ...\n",
        " - Dropping outliers\n",
        " - Imputation by replacing outliers with mean, median ...\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9Q2YustL_ZG7"
      },
      "source": [
        "\n",
        "## Weather data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fuj8QfY6hDvN",
        "outputId": "f33f461d-3dae-4e9e-bf80-91aeb3777ddd"
      },
      "outputs": [],
      "source": [
        "weather_df.lcl_datetime.min(), weather_df.lcl_datetime.max()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 518
        },
        "id": "bVcOYAgMYjji",
        "outputId": "4a36f5fb-8db4-4677-fe05-de5e5c868772"
      },
      "outputs": [],
      "source": [
        "weather_df.describe(include='all')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UNt15VEaZjPQ"
      },
      "source": [
        "Some insights from the above summary:\n",
        "\n",
        " - An average of 0.151mm of precipitation was recorded in May 2024\n",
        " - Highest precipitation was 4.34mm\n",
        " - Average temp in May was 28.30 deg Celcius with a range between 25.92 and 30.49 degrees Celcius\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QgvLRzJpbPaK"
      },
      "source": [
        "Let's merge the weather and trips data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 226
        },
        "id": "R994xR9LiRe8",
        "outputId": "b7a7917f-ccc3-4286-a3b7-3469564922fb"
      },
      "outputs": [],
      "source": [
        "train.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T_hufi5lbT2v"
      },
      "outputs": [],
      "source": [
        "# create day_hour variable to merge trips and weather data on\n",
        "train['day_hour'] = train['lcl_start_transporting_dttm_day'].astype(str).str.split('.').str[0] +\\\n",
        " '_' + train['lcl_start_transporting_dttm_hour'].astype(str).str.split('.').str[0]\n",
        "\n",
        "test['day_hour'] = test['lcl_start_transporting_dttm_day'].astype(str).str.split('.').str[0] +\\\n",
        " '_' + test['lcl_start_transporting_dttm_hour'].astype(str).str.split('.').str[0]\n",
        "\n",
        "weather_df['day_hour'] = weather_df['lcl_datetime_day'].astype(str).str.split('.').str[0] +\\\n",
        " '_' + weather_df['lcl_datetime_hour'].astype(str).str.split('.').str[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3qSvvTwDaWtJ"
      },
      "outputs": [],
      "source": [
        "train = train.merge(weather_df, on='day_hour', how='left')\n",
        "test = test.merge(weather_df, on='day_hour', how='left')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 342
        },
        "id": "PSfe6HVXiz5a",
        "outputId": "a93c58f1-c348-462e-cdd3-2f449a424eaa"
      },
      "outputs": [],
      "source": [
        "train.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 313
        },
        "id": "_eGGrpIui6h5",
        "outputId": "db9c167a-ea0d-40f7-ad7d-2e0742f9b3a1"
      },
      "outputs": [],
      "source": [
        "test.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bJNhVNKn5uEE"
      },
      "source": [
        "\n",
        "## Missing values and duplicates"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3Iw1aSYrccq6",
        "outputId": "12257fff-ddd3-4598-fb57-2049220c1abb"
      },
      "outputs": [],
      "source": [
        "# Check for missing values\n",
        "train.isnull().sum().any(), test.isnull().sum().any()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e06doaah27vk"
      },
      "source": [
        "Suggestions on how to handle missing values:\n",
        " - Fill in missing values with mode, mean, median..\n",
        " - Drop Missing datapoints with missing values\n",
        " - Fill in with a large number e.g -999999"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lnAfndzp6Otn",
        "outputId": "66221dd1-f354-4ca0-8aa9-b9a6e083a519"
      },
      "outputs": [],
      "source": [
        "# Check for duplicates\n",
        "train.duplicated().any(), test.duplicated().any()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F9O2HykxLwcJ"
      },
      "source": [
        "\n",
        "## Correlations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 554
        },
        "id": "9qvzT3B56R2l",
        "outputId": "e3dd5a68-6b6d-447e-d300-8516fba1cd8e"
      },
      "outputs": [],
      "source": [
        "# Top 20 correlated features to the target\n",
        "top20_corrs = abs(train.select_dtypes(include='number').corr()['Target']).sort_values(ascending = False).head(20)\n",
        "top20_corrs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "J-6L_Ihb6Wrk",
        "outputId": "d0ead21e-b71c-4a75-9b8c-adfb00d42834"
      },
      "outputs": [],
      "source": [
        "cols = ['destination_lat','destination_lon','origin_lat','origin_lon',\n",
        "        'str_distance_km','transporting_distance_fact_km', 'lcl_start_transporting_dttm_day',\n",
        "        'lcl_start_transporting_dttm_month', 'prev_hour_precipitation_mm', 'temperature_C', 'Target']\n",
        "\n",
        "# Plotting a heatmap to show correlations between variables\n",
        "corr = train[cols].corr()\n",
        "plt.figure(figsize = (15, 12))\n",
        "sns.heatmap(corr, cmap='RdYlGn', annot = True, center = 0)\n",
        "plt.title('Correlogram', fontsize = 15, color = 'darkgreen')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# # Linear models\n",
        "# from sklearn.linear_model import (\n",
        "#     LinearRegression, Ridge, Lasso, ElasticNet,\n",
        "#     BayesianRidge, ARDRegression,\n",
        "#     HuberRegressor, RANSACRegressor, TheilSenRegressor,\n",
        "#     SGDRegressor,\n",
        "#     PoissonRegressor, GammaRegressor, TweedieRegressor\n",
        "# )\n",
        "\n",
        "# # Ensemble models\n",
        "# from sklearn.ensemble import (\n",
        "#     RandomForestRegressor, GradientBoostingRegressor,\n",
        "#     HistGradientBoostingRegressor, AdaBoostRegressor,\n",
        "#     BaggingRegressor, ExtraTreesRegressor,\n",
        "#     StackingRegressor, VotingRegressor\n",
        "# )\n",
        "\n",
        "# # Tree-based model\n",
        "# from sklearn.tree import DecisionTreeRegressor\n",
        "\n",
        "# # Support Vector Regression\n",
        "# from sklearn.svm import SVR, LinearSVR\n",
        "\n",
        "# # Nearest Neighbors\n",
        "# from sklearn.neighbors import KNeighborsRegressor, RadiusNeighborsRegressor\n",
        "\n",
        "# # Gaussian Process Regression\n",
        "# from sklearn.gaussian_process import GaussianProcessRegressor\n",
        "\n",
        "# # Neural Networks\n",
        "# from sklearn.neural_network import MLPRegressor"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2gIDFBaMs8fk"
      },
      "source": [
        "\n",
        "## Modelling"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EsmtpdMu7Crk",
        "outputId": "2563bd34-7f6b-430f-e70f-87aa8aee4563"
      },
      "outputs": [],
      "source": [
        "# Selecting the independent variables and the target variable\n",
        "feature_cols = ['str_distance_km', 'temperature_C','transporting_distance_fact_km',\n",
        "           'lcl_start_transporting_dttm_day', 'prev_hour_precipitation_mm']\n",
        "X = train[feature_cols].fillna(0)\n",
        "y = train.Target\n",
        "\n",
        "# Splitting the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.05, random_state = SEED)\n",
        "\n",
        "# Instantiating the model\n",
        "# clf = LinearRegression(n_jobs=-1, fit_intercept=True, copy_X=True)\n",
        "clf = MLPRegressor()\n",
        "# clf = RandomForestRegressor(random_state = SEED, n_jobs=-1)\n",
        "clf.fit(X_train, y_train)\n",
        "\n",
        "# Making predictions\n",
        "y_pred = clf.predict(X_test)\n",
        "\n",
        "# Measuring the accuracy of the model\n",
        "print(f'RMSE Score: {np.sqrt(mean_squared_error(y_test, y_pred))}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 877
        },
        "id": "SICL6RDX8K7B",
        "outputId": "0436d4d6-2054-450a-8008-db96a5ae5b1a"
      },
      "outputs": [],
      "source": [
        "# Feature importance\n",
        "impo_df = pd.DataFrame({'feature': X.columns, 'importance': clf.feature_importances_}).set_index('feature').sort_values(by = 'importance', ascending = False)\n",
        "impo_df = impo_df[:12].sort_values(by = 'importance', ascending = True)\n",
        "impo_df.plot(kind = 'barh', figsize = (10, 10))\n",
        "plt.legend(loc = 'center right')\n",
        "plt.title('Bar chart showing feature importance', fontsize = 14)\n",
        "plt.xlabel('Features', fontsize = 12)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7UuShmECwAbL"
      },
      "source": [
        "\n",
        "## Making predictions of the test set and creating a submission file"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "lUel_76z8taS",
        "outputId": "d5f6b371-d5b6-492e-bdbc-c2edff454d50"
      },
      "outputs": [],
      "source": [
        "# Make prediction on the test set\n",
        "test_df = test[feature_cols].fillna(0)\n",
        "predictions = clf.predict(test_df)\n",
        "\n",
        "# # Create a submission file\n",
        "sub_file = pd.DataFrame({'trip_id': test.trip_id, 'Target': predictions})\n",
        "sub_file.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H3br7aoi8zR3"
      },
      "outputs": [],
      "source": [
        "# Create file\n",
        "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "filename = f\"MLPRegressor{timestamp}.csv\"\n",
        "sub_file.to_csv(\"submissions/\"+filename, index = False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# NEURAL NETWORK"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# !pip install tensorflow"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, models\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Enhanced feature engineering with proper handling of edge cases\n",
        "def create_enhanced_features(df):\n",
        "    df = df.copy()\n",
        "    \n",
        "    # Distance ratio (actual vs straight-line) - handle division by zero\n",
        "    df['distance_ratio'] = np.where(\n",
        "        df['str_distance_km'] > 0,\n",
        "        df['transporting_distance_fact_km'] / df['str_distance_km'],\n",
        "        1.0  # Default ratio when straight-line distance is 0\n",
        "    )\n",
        "    \n",
        "    # Cap extreme values to prevent overflow\n",
        "    df['distance_ratio'] = np.clip(df['distance_ratio'], 0, 10)  # Reasonable upper bound\n",
        "    \n",
        "    # Time-based features\n",
        "    df['hour_sin'] = np.sin(2 * np.pi * df['lcl_start_transporting_dttm_hour'] / 24)\n",
        "    df['hour_cos'] = np.cos(2 * np.pi * df['lcl_start_transporting_dttm_hour'] / 24)\n",
        "    \n",
        "    # Rush hour indicator\n",
        "    df['rush_hour'] = ((df['lcl_start_transporting_dttm_hour'] >= 7) & \n",
        "                       (df['lcl_start_transporting_dttm_hour'] <= 9) |\n",
        "                       (df['lcl_start_transporting_dttm_hour'] >= 17) & \n",
        "                       (df['lcl_start_transporting_dttm_hour'] <= 19)).astype(int)\n",
        "    \n",
        "    return df\n",
        "\n",
        "# Enhanced feature set\n",
        "enhanced_features = [\n",
        "    'str_distance_km', 'transporting_distance_fact_km', 'distance_ratio',\n",
        "    'origin_lat', 'origin_lon', 'destination_lat', 'destination_lon',\n",
        "    'hour_sin', 'hour_cos', 'rush_hour',\n",
        "    'lcl_start_transporting_dttm_day', 'temperature_C', 'prev_hour_precipitation_mm'\n",
        "]\n",
        "\n",
        "# Prepare data\n",
        "train_enhanced = create_enhanced_features(train)\n",
        "test_enhanced = create_enhanced_features(test)\n",
        "\n",
        "X_enhanced = train_enhanced[enhanced_features].fillna(0)\n",
        "y_enhanced = train_enhanced['Target']\n",
        "\n",
        "# Additional check for infinite or NaN values\n",
        "print(\"Checking for infinite or NaN values:\")\n",
        "print(f\"Infinite values: {np.isinf(X_enhanced).sum().sum()}\")\n",
        "print(f\"NaN values: {np.isnan(X_enhanced).sum().sum()}\")\n",
        "\n",
        "# Replace any remaining infinite values with 0\n",
        "X_enhanced = X_enhanced.replace([np.inf, -np.inf], 0)\n",
        "\n",
        "# Scale features\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X_enhanced)\n",
        "\n",
        "# Additional check after scaling\n",
        "print(f\"After scaling - Infinite values: {np.isinf(X_scaled).sum()}\")\n",
        "print(f\"After scaling - NaN values: {np.isnan(X_scaled).sum()}\")\n",
        "\n",
        "# Split data\n",
        "X_train_tf, X_val_tf, y_train_tf, y_val_tf = train_test_split(\n",
        "    X_scaled, y_enhanced, test_size=0.2, random_state=SEED\n",
        ")\n",
        "\n",
        "# Build and train model\n",
        "tf_model = build_tf_model(X_scaled.shape[1])\n",
        "\n",
        "# Early stopping and model checkpointing\n",
        "callbacks = [\n",
        "    tf.keras.callbacks.EarlyStopping(patience=20, restore_best_weights=True),\n",
        "    tf.keras.callbacks.ReduceLROnPlateau(factor=0.5, patience=10)\n",
        "]\n",
        "\n",
        "history = tf_model.fit(\n",
        "    X_train_tf, y_train_tf,\n",
        "    validation_data=(X_val_tf, y_val_tf),\n",
        "    epochs=200,\n",
        "    batch_size=256,\n",
        "    callbacks=callbacks,\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "# Evaluate\n",
        "val_predictions = tf_model.predict(X_val_tf)\n",
        "tf_rmse = np.sqrt(mean_squared_error(y_val_tf, val_predictions))\n",
        "print(f'TensorFlow NN RMSE: {tf_rmse:.4f}')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Make predictions on test set using TensorFlow model\n",
        "test_enhanced_features = test_enhanced[enhanced_features].fillna(0)\n",
        "\n",
        "# Replace any infinite values in test set\n",
        "test_enhanced_features = test_enhanced_features.replace([np.inf, -np.inf], 0)\n",
        "\n",
        "# Scale test features using the same scaler fitted on training data\n",
        "test_scaled = scaler.transform(test_enhanced_features)\n",
        "\n",
        "# Make predictions\n",
        "test_predictions = tf_model.predict(test_scaled)\n",
        "\n",
        "# Flatten predictions (remove extra dimension)\n",
        "test_predictions = test_predictions.flatten()\n",
        "\n",
        "# Create submission dataframe\n",
        "submission = pd.DataFrame({\n",
        "    'trip_id': test['trip_id'],\n",
        "    'Target': test_predictions\n",
        "})\n",
        "\n",
        "# Preview the submission\n",
        "print(\"Submission preview:\")\n",
        "print(submission.head())\n",
        "print(f\"\\nSubmission shape: {submission.shape}\")\n",
        "print(f\"Target predictions range: {test_predictions.min():.4f} to {test_predictions.max():.4f}\")\n",
        "\n",
        "# Create submission file with timestamp\n",
        "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "filename = f\"TensorFlow_NN_{timestamp}.csv\"\n",
        "filepath = os.path.join(SUBMISSION_PATH, filename)\n",
        "\n",
        "submission.to_csv(filepath, index=False)\n",
        "print(f\"\\nSubmission file saved as: {filename}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Random Forest with Enhanced Features - Complete Pipeline\n",
        "\n",
        "# Enhanced feature engineering with proper handling of edge cases\n",
        "def create_enhanced_features(df):\n",
        "    df = df.copy()\n",
        "    \n",
        "    # Distance ratio (actual vs straight-line) - handle division by zero\n",
        "    df['distance_ratio'] = np.where(\n",
        "        df['str_distance_km'] > 0,\n",
        "        df['transporting_distance_fact_km'] / df['str_distance_km'],\n",
        "        1.0  # Default ratio when straight-line distance is 0\n",
        "    )\n",
        "    \n",
        "    # Cap extreme values to prevent overflow\n",
        "    df['distance_ratio'] = np.clip(df['distance_ratio'], 0, 10)  # Reasonable upper bound\n",
        "    \n",
        "    # Time-based features\n",
        "    df['hour_sin'] = np.sin(2 * np.pi * df['lcl_start_transporting_dttm_hour'] / 24)\n",
        "    df['hour_cos'] = np.cos(2 * np.pi * df['lcl_start_transporting_dttm_hour'] / 24)\n",
        "    \n",
        "    # Rush hour indicator\n",
        "    df['rush_hour'] = ((df['lcl_start_transporting_dttm_hour'] >= 7) & \n",
        "                       (df['lcl_start_transporting_dttm_hour'] <= 9) |\n",
        "                       (df['lcl_start_transporting_dttm_hour'] >= 17) & \n",
        "                       (df['lcl_start_transporting_dttm_hour'] <= 19)).astype(int)\n",
        "    \n",
        "    return df\n",
        "\n",
        "# Enhanced feature set\n",
        "enhanced_features = [\n",
        "    'str_distance_km', 'transporting_distance_fact_km', 'distance_ratio',\n",
        "    'origin_lat', 'origin_lon', 'destination_lat', 'destination_lon',\n",
        "    'hour_sin', 'hour_cos', 'rush_hour',\n",
        "    'lcl_start_transporting_dttm_day', 'temperature_C', 'prev_hour_precipitation_mm'\n",
        "]\n",
        "\n",
        "# Prepare enhanced training data\n",
        "train_enhanced = create_enhanced_features(train)\n",
        "test_enhanced = create_enhanced_features(test)\n",
        "\n",
        "X_enhanced = train_enhanced[enhanced_features].fillna(0)\n",
        "y_enhanced = train_enhanced['Target']\n",
        "\n",
        "# Replace any infinite values\n",
        "X_enhanced = X_enhanced.replace([np.inf, -np.inf], 0)\n",
        "\n",
        "print(f\"Enhanced feature set shape: {X_enhanced.shape}\")\n",
        "print(f\"Features: {enhanced_features}\")\n",
        "\n",
        "# Split data for validation\n",
        "X_train_rf, X_val_rf, y_train_rf, y_val_rf = train_test_split(\n",
        "    X_enhanced, y_enhanced, test_size=0.2, random_state=SEED\n",
        ")\n",
        "\n",
        "# Train Random Forest with enhanced features\n",
        "rf_model = KNeighborsRegressor()\n",
        "\n",
        "print(\"Training Random Forest model...\")\n",
        "rf_model.fit(X_train_rf, y_train_rf)\n",
        "\n",
        "# Evaluate on validation set\n",
        "val_predictions = rf_model.predict(X_val_rf)\n",
        "rf_rmse = np.sqrt(mean_squared_error(y_val_rf, val_predictions))\n",
        "print(f'Random Forest RMSE: {rf_rmse:.4f}')\n",
        "\n",
        "# Feature importance\n",
        "# feature_importance = pd.DataFrame({\n",
        "#     'feature': enhanced_features,\n",
        "#     'importance': rf_model.feature_importances_\n",
        "# }).sort_values('importance', ascending=False)\n",
        "\n",
        "# print(\"\\nTop 10 Most Important Features:\")\n",
        "# print(feature_importance.head(10))\n",
        "\n",
        "# Prepare test data and make predictions\n",
        "test_enhanced_features = test_enhanced[enhanced_features].fillna(0)\n",
        "test_enhanced_features = test_enhanced_features.replace([np.inf, -np.inf], 0)\n",
        "\n",
        "# Make predictions on test set\n",
        "test_predictions = rf_model.predict(test_enhanced_features)\n",
        "\n",
        "# Create submission dataframe\n",
        "submission = pd.DataFrame({\n",
        "    'trip_id': test['trip_id'],\n",
        "    'Target': test_predictions\n",
        "})\n",
        "\n",
        "# Preview the submission\n",
        "print(\"\\nSubmission preview:\")\n",
        "print(submission.head())\n",
        "print(f\"\\nSubmission shape: {submission.shape}\")\n",
        "print(f\"Target predictions range: {test_predictions.min():.4f} to {test_predictions.max():.4f}\")\n",
        "\n",
        "# Create submission file with timestamp\n",
        "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "filename = f\"KNeighborsRegressor_Enhanced_{timestamp}.csv\"\n",
        "filepath = os.path.join(SUBMISSION_PATH, filename)\n",
        "\n",
        "submission.to_csv(filepath, index=False)\n",
        "print(f\"\\nSubmission file saved as: {filename}\")\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
